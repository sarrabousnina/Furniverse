# Furniverse

AI-powered furniture e-commerce platform featuring multimodal semantic search, room analysis, product comparison, and AR visualization.

![Furniverse Homepage](Frontend/images/banner.png)


---

## What is this?

A furniture recommendation system that works in **three directions**:

1. **User â†’ Product**: Natural language queries + image uploads â†’ semantic product matching
2. **Product â†’ User**: Given a product, find users who'd want it (collaborative signals)  
3. **Room â†’ Products**: Upload a room photo â†’ detect existing furniture â†’ recommend missing pieces

All powered by **multimodal embeddings** stored in **Qdrant** vector database.

---

## Table of Contents

- [Multimodal Embedding Pipeline](#multimodal-embedding-pipeline)
- [Core Architecture](#core-architecture)
- [Recommendation Modes](#recommendation-modes)
- [Tech Stack](#tech-stack)
- [Project Structure](#project-structure)
- [Setup](#setup)

---

## Multimodal Embedding Pipeline

Each product is represented by **four embedding types**, fused at query time:

| Embedding | Model | Dimension | Purpose |
|-----------|-------|-----------|---------|
| **Text** | CLIP ViT-B/32 | 512 | Natural language understanding, semantic text matching |
| **Image** | CLIP ViT-B/32 | 512 | Visual understanding, cross-modal text-image alignment |
| **Graph** | Node2Vec | 64 | Co-purchase patterns, style relationships |
| **Color** | K-means clusters | 548 | Color palette similarity (dominant colors) |

### Fusion Strategy

```
final_score = 0.30 Ã— CLIP_text_similarity 
            + 0.30 Ã— CLIP_image_similarity
            + 0.30 Ã— Graph_similarity 
            + 0.10 Ã— Color_similarity
```

Weights determined empirically. CLIP dominates because it handles both text queries and cross-modal matching.

### Why This Combination?

- **CLIP alone** misses behavioral patterns (what people actually buy together)
- **Graph alone** has cold-start problem for new products
- **Color alone** ignores semantic meaning ("modern blue sofa" vs just "blue")
- **Together**: semantic understanding + behavioral signals + visual aesthetics

---

## Core Architecture

![System Architecture](Frontend/images/system-architecture.png)

The system consists of two main components:

### Data Pipeline (Offline Processing)
1. **Data Collection**: Scraping IKEA product catalog 
2. **Synthetic Data Generation**: Creating realistic user interactions and purchase patterns using semantic search
3. **Multi-embedding Generation**: CLIP text/image vectors, Node2Vec graph embeddings, color features
4. **Vector Storage**: All embeddings stored in Qdrant with named vectors for efficient retrieval

### Runtime Architecture (Online Processing)
- **Bidirectional Recommendation Engine**: Handles Userâ†’Product and Productâ†’User matching
- **Room Analysis Module**: YOLO-based furniture detection with upgrade recommendations
- **3D Model Generation**: Tripo AI integration for AR-ready GLB models
- **Product Comparison**: AI-powered side-by-side analysis with trade-off explanations

---

## Multimodal Embedding Pipeline

Each product is represented by **three embedding types**, fused at query time:

| Embedding | Model | Dimension | Purpose |
|-----------|-------|-----------|---------|
| **Text** | CLIP ViT-B/32 | 512 | Natural language understanding, semantic text matching |
| **Image** | CLIP ViT-B/32 | 512 | Visual understanding, cross-modal text-image alignment |
| **Graph** | Node2Vec | 64 | Co-purchase patterns, style relationships |
| **Color** | K-means clusters | 548 | Color palette similarity (dominant colors) |

### Fusion Strategy

```
final_score = 0.30 Ã— CLIP_text_similarity 
            + 0.30 Ã— CLIP_image_similarity
            + 0.30 Ã— Graph_similarity 
            + 0.10 Ã— Color_similarity
```

Weights determined empirically. CLIP dominates because it handles both text queries and cross-modal matching.

### Why This Combination?

- **CLIP alone** misses behavioral patterns (what people actually buy together)
- **Graph alone** has cold-start problem for new products
- **Color alone** ignores semantic meaning ("modern blue sofa" vs just "blue")
- **Together**: semantic understanding + behavioral signals + visual aesthetics

---

## Recommendation Modes

### 1. Text â†’ Product (Smart Search)

User types: `"comfortable leather sofa under $800"`

**Pipeline:**
1. CLIP encodes query â†’ 512D vector
2. Qdrant searches `furniture_products` collection
3. Embedding-based attribute extraction (no regex):
   - Material: cosine similarity to `["leather", "velvet", "fabric"]` embeddings
   - Budget: parsed from query
   - Style: similarity to `["modern", "scandinavian", "industrial"]`
4. Post-filter by extracted constraints
5. If budget too tight â†’ return trade-off alternatives with explanations

```python
# Semantic attribute extraction (not keyword matching)
material_scores = {
    "leather": cosine_sim(query_embedding, leather_embedding),
    "velvet": cosine_sim(query_embedding, velvet_embedding),
    ...
}
detected_material = argmax(material_scores) if max > threshold else None
```

### 2. Image â†’ Product (Visual Search)

User uploads a photo of furniture they like.

**Pipeline:**
1. CLIP encodes image â†’ 512D vector (same space as text!)
2. Qdrant vector search on product image embeddings
3. Return top-K visually similar products

Cross-modal works because CLIP aligns text and images in shared embedding space.

### 3. Room â†’ Products (Room Analysis)

User uploads room photo.

**Pipeline:**
1. **YOLO** detects furniture objects in image (bed, sofa, table, chair, lamp)
2. Rule-based room type classification from detected objects
3. Identify missing essential furniture for room type
4. For each detected item:
   - Crop bounding box
   - CLIP encode cropped region
   - Find similar products in catalog
5. Return: detected items + upgrade suggestions + missing essentials

```python
room_essentials = {
    "living_room": {
        "required": ["sofa", "coffee table"],
        "recommended": ["lamp", "tv stand", "bookshelf"]
    },
    "bedroom": {
        "required": ["bed", "nightstand"],
        "recommended": ["dresser", "lamp", "wardrobe"]
    }
}
```

### 4. Product Comparison (AI-Powered)

Compare two products side-by-side:
- Visual similarity score (CLIP)
- Price analysis with value verdict
- Feature diff
- AI recommendation on which to choose

---

## Tech Stack

### Embeddings & Search
- **Qdrant Cloud** - Vector database with payload filtering
- **CLIP** (openai/clip-vit-base-patch32) - 512D multimodal embeddings
- **Node2Vec** - Graph embeddings from co-purchase network
- **Sentence-Transformers** - Text encoding utilities

### Computer Vision
- **YOLOv8** - Custom trained model for furniture detection and classification
- **Pillow** - Image preprocessing
- **scikit-learn** - K-means color clustering

### Backend
- **FastAPI** - Async API with Pydantic validation
- **PyTorch** - Model inference
- **Transformers** - HuggingFace model loading

### Frontend
- **React 18** + Vite
- **model-viewer** - WebXR AR preview
- **CSS Modules**

### 3D/AR
- **Tripo AI** - Image â†’ 3D model generation
- **Google Model Viewer** - AR on mobile (ARCore/ARKit)

---

## Project Structure

```
Furniverse/
â”œâ”€â”€ Backend/                 # FastAPI server & AI services
â”‚   â”œâ”€â”€ analytics/           # User behavior tracking
â”‚   â”œâ”€â”€ assets/              # Static files
â”‚   â”œâ”€â”€ config/              # Configuration files
â”‚   â”œâ”€â”€ services/            # Core business logic
â”‚   â”œâ”€â”€ tests/               # Unit & integration tests
â”‚   â”œâ”€â”€ utils/               # Helper functions
â”‚   â”œâ”€â”€ temp_images/         # Uploaded images storage
â”‚   â”œâ”€â”€ main.py              # FastAPI application
â”‚   â””â”€â”€ requirements.txt     # Python dependencies
â”‚
â”œâ”€â”€ Frontend/               # React application
â”‚   â””â”€â”€ src/
â”‚       â”œâ”€â”€ components/      # Reusable UI components
â”‚       â”œâ”€â”€ context/         # React context providers
â”‚       â”œâ”€â”€ data/            # Static data files
â”‚       â”œâ”€â”€ pages/           # Route components
â”‚       â”œâ”€â”€ services/        # API communication
â”‚       â”œâ”€â”€ utils/           # Helper functions
â”‚       â””â”€â”€ App.jsx          # Main component
â”‚
â”œâ”€â”€ Pipeline/               # ML pipeline & embeddings
â”‚   â”œâ”€â”€ cv/                  # Computer vision models
â”‚   â”œâ”€â”€ embeddings/          # CLIP & embedding utils
â”‚   â”œâ”€â”€ graph/               # Node2Vec graph processing
â”‚   â”œâ”€â”€ indexing/            # Qdrant vector indexing
â”‚   â”œâ”€â”€ preprocessing/       # Data cleaning & prep
â”‚   â”œâ”€â”€ scraping/            # Web scraping tools
â”‚   â”œâ”€â”€ synthetic/           # Synthetic data generation
â”‚   â””â”€â”€ run_indexing.py     # Main indexing script
â”‚
â”œâ”€â”€ Data/                   # Dataset storage
â”‚   â”œâ”€â”€ processed/           # Clean, processed data
â”‚   â””â”€â”€ raw/                 # Raw scraped data
â”‚
â”œâ”€â”€ pyproject.toml          # Python project config
â”œâ”€â”€ uv.lock                 # Dependency lock file
â””â”€â”€ README.md               # This file
```

---

## Setup

### Requirements
- Python 3.12+
- Node.js 18+
- Qdrant Cloud account (free tier works)

### 1. Clone & Install

```bash
git clone https://github.com/sarrabousnina/Furniverse.git
cd furniverse

# Backend
pip install -r Backend/requirements.txt
pip install -r Pipeline/requirements.txt

# Frontend
cd Frontend && npm install
```

### 2. Configure Qdrant

Create `Pipeline/qdrant_config.py`:
```python
QDRANT_URL = "https://xxx.qdrant.io"
QDRANT_API_KEY = "your-key"
```

### 3. Index Products

```bash
cd Pipeline
python run_indexing.py
```

Generates embeddings for all products (~200). Takes ~5 min on CPU.

### 4. Run

```bash
# Terminal 1: Backend
cd Backend
uvicorn main:app --reload --port 8000

# Terminal 2: Frontend  
cd Frontend
npm run dev
```

- API: http://localhost:8000/docs
- UI: http://localhost:5173


---

## ðŸ‘¥ Team Members

- **[Marwa Mokhtar]**
- **[Roua Khalfet]**
- **[Sarra Bousnina]**
- **[Sameur Mkaouar]**
- **[Yassine Kharrat]**

---
